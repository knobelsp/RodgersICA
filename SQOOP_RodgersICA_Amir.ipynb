{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import erf\n",
    "import netCDF4\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "\n",
    "\n",
    "Dimensions specified below\n",
    "\n",
    "$\\textit{m}$: length of the measurement vector\n",
    "\n",
    "$\\textit{n}$: length of the state (parameter) vector\n",
    "\n",
    "$\\textit{r}$: length of the model error vector\n",
    "\n",
    "\n",
    "To perform this ICA, you'll need a few things:\n",
    "\n",
    "1. Jacobian matrix, defined as the $ K_{j,i}(\\textbf{x}) = \\frac{\\partial F_i (\\textbf{x})}{\\partial x_j}$ where $\\textit{i}$ has length $\\textit{m}$ and $\\textit{j}$ has length $\\textit{n}$. $F(\\textbf{x})$ is the forward model at the state defined by $\\textbf{x}$.\n",
    "\n",
    "    jac [n x m]\n",
    "    \n",
    "2. Model error Jacobian matrix, defined as the $ K_{b,u,i}(\\textbf{x}) = \\frac{\\partial F_i (\\textbf{x})}{\\partial b_u}$ where $\\textit{i}$ has length $\\textit{m}$ and $\\textit{u}$ has length $\\textit{r}$. $F(\\textbf{x})$ is the forward model at the state defined by $\\textbf{x}$. Note there are options in the Rodgers function to omit consideration of model errors if these are not known.\n",
    "\n",
    "    jac_me [r x m]\n",
    "    \n",
    "3. Measurement error coveriance matrix, $S_{\\epsilon}$. This can be specificed as either an $\\textit{m}$ length vector of sigma squared measurement uncertainties or an $\\textit{m} x \\textit{m}$ full covariance matrix. In the former case, it is assumed that measurement uncertainty is uncorrelated. \n",
    "\n",
    "    err [m x m] or [m]   \n",
    "\n",
    "4. Model error coveriance matrix, $S_{b}$. This can be specificed as either an $\\textit{r}$ length vector of sigma squared measurement uncertainties or an $\\textit{r} x \\textit{r}$ full covariance matrix. In the former case, it is assumed that measurement uncertainty is uncorrelated. Note there are options below to omit consideration of model errors if these are not known.\n",
    "\n",
    "    err_me [m x m] or [m]   \n",
    "    \n",
    "5. A priori error coveriance matrix, $S_{a}$. This can be specificed as either an $\\textit{n}$ length vector of sigma squared measurement uncertainties or an $\\textit{n} x \\textit{n}$ full covariance matrix. In the former case, it is assumed that measurement uncertainty is uncorrelated. \n",
    "\n",
    "    ap [n x n] or [n]   \n",
    "    \n",
    "6. If calculating the detection probability: the value of the parameter in question\n",
    "\n",
    "    mu [scalar]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outputs\n",
    "\n",
    "\n",
    "### from rodgers()\n",
    "1. Error covariance matrix, $\\hat{S}$.\n",
    "\n",
    "    S_hat [n x n]\n",
    "    \n",
    "2. Shannon Information Content, $SIC$.\n",
    "\n",
    "    SIC [scalar]\n",
    "    \n",
    "3. Averaging kernel matrix, $A$. \n",
    "\n",
    "    AvgK [n x n]\n",
    "\n",
    "4. Degrees of Freedom for Signal, $DFS$. \n",
    "\n",
    "    DFS [scalar]  \n",
    "\n",
    "    \n",
    "### from detect_prob()\n",
    "1. Probability of detection, $P_d$. Note this requires inputs mu (parameter value in question) and sigma ($\\sqrt{\\hat{S}_{p,p}}$ where $p$ is the parameter index.)\n",
    "\n",
    "    Pd [scalar]\n",
    "    \n",
    "    Pd_pcnt_str [percentage as string]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from Amir:\n",
    "    Relative:\n",
    "σ_sys = np.array([0.01587401, 0.01244266, 0.00938955, 0.01092051, 0.00302288,\n",
    "                  0.00544271, 0.00999068, 0.01467843, 0.0080387, 0.00944394,\n",
    "                  0.0193447, 0.0224503, 0.02386379])\n",
    "\n",
    "Absolute:\n",
    "σ_sys = np.array([2.80453033e-03, 1.52152435e-03, 1.04028473e-03, 9.98138237e-04,\n",
    "                  1.12356970e-04, 2.68196657e-04, 4.69710954e-04, 4.32004600e-04,\n",
    "                  2.49862337e-04, 2.53730871e-04, 8.83851738e-05, 1.47447273e-04,\n",
    "                  1.57220696e-04])\n",
    "\n",
    "This is based on real modisa data using our OE algorithm, where we use the full spectrum to optimize things, which means that these values work better when you use all bands in MODIS. The systematic uncertainty dominates the random noise, so I feel comfortable ignoring the random component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate the parameter error covariance matrix.\n",
    "\n",
    "#input Jacobian, K, [n x m], error covariance matrix Se, [m x m] and a priori matrix Sa, [n x n]\n",
    "#jac_me and me are associated with model uncertainty - that parameterized uncertainty and its jacobian\n",
    "def rodgers(jac, err, ap, model_error={}, model_error_jacobian={}): \n",
    "    #todo\n",
    "    # consider microplastic simulation jacobian in the following manner: Se' = Se + Kb Sb Kbt, where \n",
    "    #  Se is the same as above, Sb is the microplastic parameter uncertainty, Kb the microplastic jacobian\n",
    "    \n",
    "        #check if error covariance matrix is square, or just diagonal values. If latter make full matrix\n",
    "    if err.ndim == 1:\n",
    "        ln=np.shape(err)\n",
    "        err2d = np.zeros((ln[0], ln[0]))\n",
    "        np.fill_diagonal(err2d, err)\n",
    "        err=err2d\n",
    "\n",
    "        #check if a priori covariance matrix is square, or just diagonal values. If latter make full matrix\n",
    "    if ap.ndim == 1:\n",
    "        ln=np.shape(ap)\n",
    "        ap2d = np.zeros((ln[0], ln[0]))\n",
    "        np.fill_diagonal(ap2d, ap)\n",
    "        ap=ap2d        \n",
    "            \n",
    "        #section to verify compatable dimensions ------------------------------------------------------\n",
    "    sh_jac = np.shape(jac)\n",
    "    sh_err = np.shape(err)\n",
    "    sh_ap = np.shape(ap)\n",
    "    \n",
    "    n_dim = sh_jac[0]\n",
    "    m_dim = sh_jac[1]\n",
    "    \n",
    "    if not((sh_err[0] == sh_err[1]) and (sh_ap[0] == sh_ap[1])):\n",
    "        print('ERROR: error covariance matrix or a priori matrix are not square')\n",
    "        print('Error covariance matrix dimensions')\n",
    "        print(sh_err)\n",
    "        print('A priori matrix dimensions')\n",
    "        print(sh_ap)\n",
    "        return -1, -1, -1, -1\n",
    "    \n",
    "    if not(sh_jac[0] == sh_ap[0]):\n",
    "        print('ERROR: n dimensions inconsistent, should be Jacobian [n x m]; a priori [n x n]')\n",
    "        print('Jacobian matrix dimensions')\n",
    "        print(sh_jac)\n",
    "        print('A priori matrix dimensions')\n",
    "        print(sh_ap)\n",
    "        return -1, -1, -1, -1\n",
    "    \n",
    "    if not(sh_jac[1] == sh_err[0]):\n",
    "        print('ERROR: m dimensions inconsistent, should be Jacobian [n x m]; error covariance [m x m]')\n",
    "        print('Jacobian matrix dimensions')\n",
    "        print(sh_jac)\n",
    "        print('Error covariance matrix dimensions')\n",
    "        print(sh_err)\n",
    "        return -1, -1, -1, -1\n",
    "        \n",
    "    #section to generate model derived error -------------------------------------------------------\n",
    "    \n",
    "    if len(model_error) > 0:\n",
    "        me=model_error\n",
    "        jac_me=model_error_jacobian\n",
    "        \n",
    "        ln_me=np.shape(me)\n",
    "        errme_2d = np.zeros((ln_me[0], ln_me[0]))\n",
    "        np.fill_diagonal(errme_2d, me)\n",
    "        err_me=errme_2d\n",
    "    \n",
    "        jac_me_t=np.transpose(jac_me)      \n",
    "    \n",
    "        JacmetMeJacme = np.matmul(jac_me_t,np.matmul(err_me,jac_me))\n",
    "        err = err + JacmetMeJacme\n",
    "    \n",
    "        #perform inverse and matrix multiplication calculations ----------------------------------------\n",
    "    jac_t=np.transpose(jac) #transpose of Jacobian (KT)\n",
    "    \n",
    "    try: \n",
    "        err_i=np.linalg.inv(err) #inverse of error covariance matrix (Se-1)\n",
    "    except:\n",
    "        print(\"ERROR: problem inverting error covariance matrix\")\n",
    "        return -1, -1, -1, -1\n",
    "    \n",
    "    try: \n",
    "        ap_i=np.linalg.inv(ap) #inverse of a priori error covariance matrix\n",
    "    except:\n",
    "        print(\"ERROR: problem inverting a priori covariance matrix\")\n",
    "        return -1, -1, -1, -1\n",
    "\n",
    "    KtSK = np.matmul(jac,np.matmul(err_i,jac_t)) #calcuates KT Se-1 K\n",
    "\n",
    "    try: \n",
    "        S_hat = np.linalg.inv(KtSK+ap_i) #calculate the inverse of (above + Sa-1)\n",
    "    except:\n",
    "        print(\"ERROR: problem inverting retrieval error covariance matrix\")\n",
    "        return -1, -1, -1, -1\n",
    "    \n",
    "    SIC = 0.5*np.log(np.linalg.det(np.matmul((KtSK+ap_i),ap))) #calculate Shannon Information Content    \n",
    "    AvgK = np.matmul(S_hat,KtSK) #averaging kernel\n",
    "    DFS = np.trace(AvgK) #degrees of freedom for signal (DFS) which is trace of averaging kernel\n",
    "    \n",
    "    return S_hat, SIC, AvgK, DFS  #returns retrieval error covariance matrix and the Shannon Information Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates the probability of detection given the parameter value (mu) and uncertainty (sigma)\n",
    "#assumes PDF is gaussian normally distributed\n",
    "def detect_prob(mu, sigma, doprint=0): \n",
    "\n",
    "    Pd = 1-0.5*(1+erf((-1*mu)/(sigma*np.sqrt(2))))  #detection probability, modified from CDF function\n",
    "\n",
    "    Pd_pcnt_str=str(np.around(Pd*100,decimals=1))+'% positive probability' #string output version\n",
    "\n",
    "    if doprint > 0:\n",
    "        print(Pd_pcnt_str)\n",
    "\n",
    "    return Pd, Pd_pcnt_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate detection probability metrics for a full range of fractional plastic coverage. \n",
    "#Also, return result at 95% or whatever is specified in fraction_threshold variable\n",
    "def detect_prob_all(plastic_uncertainty,plastic_fraction,fraction_threshold=0.95):    \n",
    "\n",
    "    #make array of values to assess (val) and dummy array to fill (det_prob)\n",
    "    inc=np.arange(0, 10000, 1)\n",
    "    val=inc/10000\n",
    "    det_prob=np.arange(0, 10000, 1) / 10000\n",
    "       \n",
    "    #interpolate plastic_uncertainty to assessment values\n",
    "    plastic_uncertainty_int = np.interp(val,plastic_fraction,plastic_uncertainty)    \n",
    "    \n",
    "    for x in inc:\n",
    "        Pd, Pd_pcnt_str = detect_prob(val[x], plastic_uncertainty_int[x], doprint=0)\n",
    "        det_prob[x] = Pd\n",
    "\n",
    "    #get plastic fraction for a fraction_threshold detection probability \n",
    "    fraction_meeting_threshold = np.interp(fraction_threshold,det_prob,val)\n",
    "    \n",
    "    return det_prob, fraction_meeting_threshold\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out(S_hat, SIC, AvgK, DFS, jac, err, ap, me_err, numpts, params, me_params ):\n",
    "\n",
    "    S_hat_diag=np.diagonal(S_hat)\n",
    "    uncert=np.sqrt(S_hat_diag)\n",
    "\n",
    "    np.set_printoptions(formatter={'float': '{: 0.2f}'.format})\n",
    "    print('Error covariance matrix:')\n",
    "    print(S_hat)\n",
    "    print()\n",
    "\n",
    "    np.set_printoptions(formatter={'float': '{: 0.5f}'.format})\n",
    "    print('Averaging kernel matrix:')\n",
    "    print(AvgK)\n",
    "    print()\n",
    "    np.set_printoptions(formatter={'float': '{: 0.5f}'.format})\n",
    "    print('Model Parameters:       ', params)\n",
    "    print('Number of observations: ', numpts)\n",
    "    print('A priori uncertainty:   ', np.sqrt(ap))\n",
    "    print('Uncertainties:          ', uncert)\n",
    "    print('Shannon Information Content:      ', SIC)\n",
    "    print('Degrees of freedom for signal:    ', DFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section to read simulated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read netcdf4 file with simulation\n",
    "#f = netCDF4.Dataset('simulations/Amir/plastics_toa_simulations_modisa_permutations.nc')\n",
    "#f = netCDF4.Dataset('simulations/Amir/plastics_toa_simulations_modisa_permutations_v3.nc')\n",
    "f = netCDF4.Dataset('./simulations/Amir/plastics_toa_simulations_modisa_permutations_v4.nc')\n",
    "\n",
    "#read jacobian and data into jac and meas, respectively. Also get z (#cases,), m (measurent), n (param) lengths\n",
    "jac_all=np.asarray(f.variables['K_Jac'])\n",
    "meas=np.asarray(f.variables['rhot'])\n",
    "param_order=np.asarray(f.variables['parameter'])\n",
    "waveln=np.asarray(f.variables['wavelength'])\n",
    "g=jac_all.shape\n",
    "z_len=g[0]\n",
    "m_len=g[1]\n",
    "n_len=g[2]\n",
    "\n",
    "#read the simulation specific parameters into a dataframe\n",
    "df = pd.DataFrame({'Windspeed(m_s)': np.asarray(f.variables['Windspeed(m_s)']),\n",
    "                   'Humidity(%)': np.asarray(f.variables['Humidity(%)']),\n",
    "                   'FMF': np.asarray(f.variables['FMF']),\n",
    "                   'AOD(869)': np.asarray(f.variables['AOD(869)']),\n",
    "                   'chla(mg_m3)': np.asarray(f.variables['chla(mg_m3)']),\n",
    "                   'plastic_fraction': np.asarray(f.variables['plastic_fraction']), \n",
    "                   'solz': np.asarray(f.variables['solz']),     \n",
    "                   'relaz': np.asarray(f.variables['relaz']),  \n",
    "                   'senz': np.asarray(f.variables['senz']),  \n",
    "                  })\n",
    "\n",
    "df[\"plastic_uncertainty\"] = np.nan\n",
    "df[\"SIC\"] = np.nan\n",
    "df[\"plastic_avgK\"] = np.nan\n",
    "df[\"DFS\"] = np.nan\n",
    "\n",
    "#close netcdf file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a priori covariance matrix\n",
    "\n",
    "#ap=(np.linspace(10.0,10.0,4))**2 #generate a priori error covariance matrix\n",
    "WS_range=df['Windspeed(m_s)'].max() - df['Windspeed(m_s)'].min()\n",
    "RH_range=df['Humidity(%)'].max() - df['Humidity(%)'].min()\n",
    "FMF_range=df['FMF'].max() - df['FMF'].min()\n",
    "AOD_range=df['AOD(869)'].max() - df['AOD(869)'].min()\n",
    "CHL_range=df['chla(mg_m3)'].max() - df['chla(mg_m3)'].min()\n",
    "PF_range=df['plastic_fraction'].max() - df['plastic_fraction'].min()\n",
    "\n",
    "\n",
    "ap=np.asarray([WS_range,RH_range,FMF_range,AOD_range,CHL_range,PF_range]) #generate a priori error covariance matrix\n",
    "ap=(ap/2)**2 #generate a priori error covariance matrix diagonals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make error covariance matrix and prepare jacobian\n",
    "\n",
    "rel_err = np.array([0.01587401, 0.01244266, 0.00938955, 0.01092051, 0.00302288,\n",
    "    0.00544271, 0.00999068, 0.01467843, 0.0080387, 0.00944394,\n",
    "    0.0193447, 0.0224503, 0.02386379])\n",
    "\n",
    "for idx in range(0, z_len):\n",
    "\n",
    "#for idx in range(29158, z_len):\n",
    "#for idx in range(0, 1000):\n",
    "\n",
    "    #rel_err=0.003  #relative error\n",
    "    #sys_err=0.005\n",
    "    #err=((meas[idx]*rel_err) + sys_err)**2 #generate error covariance matrix diagonals (code also takes 2d input)\n",
    "        \n",
    "    err=(meas[idx]*rel_err)**2\n",
    "    \n",
    "    #prepare jacobian\n",
    "    this_jac=jac_all[idx]\n",
    "    jac=this_jac.transpose()\n",
    "\n",
    "    #check for NaN in jacobian\n",
    "    if np.isnan(this_jac).any(): \n",
    "        print('Nan found in index ',idx)\n",
    "    if np.isinf(this_jac).any(): \n",
    "        print('Inf found in index ',idx)\n",
    "    \n",
    "    #calculate rodgers stuff\n",
    "    S_hat, SIC, AvgK, DFS = rodgers(jac, err, ap)\n",
    "\n",
    "    df.loc[idx][\"plastic_uncertainty\"]=np.sqrt(S_hat[5,5]) \n",
    "    df.loc[idx][\"SIC\"]=SIC\n",
    "    df.loc[idx][\"plastic_avgK\"]=AvgK[5,5]\n",
    "    df.loc[idx][\"DFS\"]=DFS\n",
    "\n",
    "df_orig=df.copy()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 131220\n",
      "1000 of 131220\n",
      "2000 of 131220\n",
      "3000 of 131220\n",
      "4000 of 131220\n",
      "5000 of 131220\n",
      "6000 of 131220\n",
      "7000 of 131220\n",
      "8000 of 131220\n",
      "9000 of 131220\n",
      "10000 of 131220\n",
      "11000 of 131220\n",
      "12000 of 131220\n",
      "13000 of 131220\n",
      "14000 of 131220\n",
      "15000 of 131220\n",
      "16000 of 131220\n",
      "17000 of 131220\n",
      "18000 of 131220\n",
      "19000 of 131220\n",
      "20000 of 131220\n",
      "21000 of 131220\n",
      "22000 of 131220\n",
      "23000 of 131220\n",
      "24000 of 131220\n",
      "25000 of 131220\n",
      "26000 of 131220\n",
      "27000 of 131220\n",
      "28000 of 131220\n",
      "29000 of 131220\n",
      "30000 of 131220\n",
      "31000 of 131220\n",
      "32000 of 131220\n",
      "33000 of 131220\n",
      "34000 of 131220\n",
      "35000 of 131220\n",
      "36000 of 131220\n",
      "37000 of 131220\n",
      "38000 of 131220\n",
      "39000 of 131220\n",
      "40000 of 131220\n",
      "41000 of 131220\n",
      "42000 of 131220\n",
      "43000 of 131220\n",
      "44000 of 131220\n",
      "45000 of 131220\n",
      "46000 of 131220\n",
      "47000 of 131220\n",
      "48000 of 131220\n",
      "49000 of 131220\n",
      "50000 of 131220\n",
      "51000 of 131220\n",
      "52000 of 131220\n",
      "53000 of 131220\n",
      "54000 of 131220\n",
      "55000 of 131220\n",
      "56000 of 131220\n",
      "57000 of 131220\n",
      "58000 of 131220\n",
      "59000 of 131220\n",
      "60000 of 131220\n",
      "61000 of 131220\n",
      "62000 of 131220\n",
      "63000 of 131220\n",
      "64000 of 131220\n",
      "65000 of 131220\n",
      "66000 of 131220\n",
      "67000 of 131220\n",
      "68000 of 131220\n",
      "69000 of 131220\n",
      "70000 of 131220\n",
      "71000 of 131220\n",
      "72000 of 131220\n",
      "73000 of 131220\n",
      "74000 of 131220\n",
      "75000 of 131220\n",
      "76000 of 131220\n",
      "77000 of 131220\n",
      "78000 of 131220\n",
      "79000 of 131220\n",
      "80000 of 131220\n",
      "81000 of 131220\n",
      "82000 of 131220\n",
      "83000 of 131220\n",
      "84000 of 131220\n",
      "85000 of 131220\n",
      "86000 of 131220\n",
      "87000 of 131220\n",
      "88000 of 131220\n",
      "89000 of 131220\n",
      "90000 of 131220\n",
      "91000 of 131220\n",
      "92000 of 131220\n",
      "93000 of 131220\n",
      "94000 of 131220\n",
      "95000 of 131220\n",
      "96000 of 131220\n",
      "97000 of 131220\n",
      "98000 of 131220\n",
      "99000 of 131220\n",
      "100000 of 131220\n",
      "101000 of 131220\n",
      "102000 of 131220\n",
      "103000 of 131220\n",
      "104000 of 131220\n",
      "105000 of 131220\n",
      "106000 of 131220\n",
      "107000 of 131220\n",
      "108000 of 131220\n",
      "109000 of 131220\n",
      "110000 of 131220\n",
      "111000 of 131220\n",
      "112000 of 131220\n",
      "113000 of 131220\n",
      "114000 of 131220\n",
      "115000 of 131220\n",
      "116000 of 131220\n",
      "117000 of 131220\n",
      "118000 of 131220\n",
      "119000 of 131220\n",
      "120000 of 131220\n",
      "121000 of 131220\n",
      "122000 of 131220\n",
      "123000 of 131220\n",
      "124000 of 131220\n",
      "125000 of 131220\n",
      "126000 of 131220\n",
      "127000 of 131220\n",
      "128000 of 131220\n",
      "129000 of 131220\n",
      "130000 of 131220\n",
      "131000 of 131220\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Windspeed(m_s)</th>\n",
       "      <th>Humidity(%)</th>\n",
       "      <th>FMF</th>\n",
       "      <th>AOD(869)</th>\n",
       "      <th>chla(mg_m3)</th>\n",
       "      <th>plastic_threshold</th>\n",
       "      <th>solz</th>\n",
       "      <th>relaz</th>\n",
       "      <th>senz</th>\n",
       "      <th>plastic_unc_median</th>\n",
       "      <th>SIC_median</th>\n",
       "      <th>plastic_avgK_median</th>\n",
       "      <th>DFS_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.029137</td>\n",
       "      <td>15.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.017002</td>\n",
       "      <td>18.288161</td>\n",
       "      <td>0.998843</td>\n",
       "      <td>3.941760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007064</td>\n",
       "      <td>15.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>16.612481</td>\n",
       "      <td>0.999916</td>\n",
       "      <td>3.960913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.005750</td>\n",
       "      <td>15.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.003754</td>\n",
       "      <td>15.838545</td>\n",
       "      <td>0.999943</td>\n",
       "      <td>3.783445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006187</td>\n",
       "      <td>15.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.004024</td>\n",
       "      <td>17.041476</td>\n",
       "      <td>0.999935</td>\n",
       "      <td>3.937373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006523</td>\n",
       "      <td>15.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.004238</td>\n",
       "      <td>16.600650</td>\n",
       "      <td>0.999928</td>\n",
       "      <td>3.959462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13116</th>\n",
       "      <td>10.0</td>\n",
       "      <td>94.9</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.009592</td>\n",
       "      <td>60.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.005840</td>\n",
       "      <td>10.817911</td>\n",
       "      <td>0.999864</td>\n",
       "      <td>3.113933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13117</th>\n",
       "      <td>10.0</td>\n",
       "      <td>94.9</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.012989</td>\n",
       "      <td>60.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.008115</td>\n",
       "      <td>10.465821</td>\n",
       "      <td>0.999737</td>\n",
       "      <td>2.870861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13118</th>\n",
       "      <td>10.0</td>\n",
       "      <td>94.9</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.019695</td>\n",
       "      <td>60.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.011919</td>\n",
       "      <td>9.901579</td>\n",
       "      <td>0.999432</td>\n",
       "      <td>3.043040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13119</th>\n",
       "      <td>10.0</td>\n",
       "      <td>94.9</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.010503</td>\n",
       "      <td>60.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.006410</td>\n",
       "      <td>11.091894</td>\n",
       "      <td>0.999836</td>\n",
       "      <td>3.291878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13120</th>\n",
       "      <td>10.0</td>\n",
       "      <td>94.9</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.016464</td>\n",
       "      <td>60.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.009805</td>\n",
       "      <td>10.790673</td>\n",
       "      <td>0.999615</td>\n",
       "      <td>3.221544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13121 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Windspeed(m_s)  Humidity(%)   FMF  AOD(869)  chla(mg_m3)  \\\n",
       "0                 0.5         30.1  0.01      0.04         0.05   \n",
       "1                 0.5         30.1  0.01      0.04         0.05   \n",
       "2                 0.5         30.1  0.01      0.04         0.05   \n",
       "3                 0.5         30.1  0.01      0.04         0.05   \n",
       "4                 0.5         30.1  0.01      0.04         0.05   \n",
       "...               ...          ...   ...       ...          ...   \n",
       "13116            10.0         94.9  0.95      0.30         2.00   \n",
       "13117            10.0         94.9  0.95      0.30         2.00   \n",
       "13118            10.0         94.9  0.95      0.30         2.00   \n",
       "13119            10.0         94.9  0.95      0.30         2.00   \n",
       "13120            10.0         94.9  0.95      0.30         2.00   \n",
       "\n",
       "       plastic_threshold  solz  relaz  senz  plastic_unc_median  SIC_median  \\\n",
       "0               0.029137  15.0   40.0  15.0            0.017002   18.288161   \n",
       "1               0.007064  15.0   40.0  30.0            0.004577   16.612481   \n",
       "2               0.005750  15.0   40.0  60.0            0.003754   15.838545   \n",
       "3               0.006187  15.0  110.0  15.0            0.004024   17.041476   \n",
       "4               0.006523  15.0  110.0  30.0            0.004238   16.600650   \n",
       "...                  ...   ...    ...   ...                 ...         ...   \n",
       "13116           0.009592  60.0  110.0  15.0            0.005840   10.817911   \n",
       "13117           0.012989  60.0  110.0  30.0            0.008115   10.465821   \n",
       "13118           0.019695  60.0  110.0  60.0            0.011919    9.901579   \n",
       "13119           0.010503  60.0  170.0  15.0            0.006410   11.091894   \n",
       "13120           0.016464  60.0  170.0  30.0            0.009805   10.790673   \n",
       "\n",
       "       plastic_avgK_median  DFS_median  \n",
       "0                 0.998843    3.941760  \n",
       "1                 0.999916    3.960913  \n",
       "2                 0.999943    3.783445  \n",
       "3                 0.999935    3.937373  \n",
       "4                 0.999928    3.959462  \n",
       "...                    ...         ...  \n",
       "13116             0.999864    3.113933  \n",
       "13117             0.999737    2.870861  \n",
       "13118             0.999432    3.043040  \n",
       "13119             0.999836    3.291878  \n",
       "13120             0.999615    3.221544  \n",
       "\n",
       "[13121 rows x 13 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#iterate through each set of conditions that have all the same parameter value except for plastic fraction. \n",
    "#create a new dataframe, and save one row for each set, with median values for plastic_uncertainty, SIC, plastic_avgK and DFS\n",
    "#also, assess detection probability and save the plastic fraction value for 90% confident probability.\n",
    "\n",
    "df.sort_values(by=['Windspeed(m_s)','Humidity(%)','FMF','AOD(869)','chla(mg_m3)','solz','relaz','senz'], inplace=True)\n",
    "\n",
    "df.rename(columns={'plastic_fraction': 'plastic_threshold', 'plastic_uncertainty': 'plastic_unc_median', 'SIC': 'SIC_median', \\\n",
    "                   'plastic_avgK':'plastic_avgK_median', 'DFS': 'DFS_median'}, inplace=True)\n",
    "\n",
    "\n",
    "fdf = df.copy()\n",
    "fdfe = fdf[0:0]\n",
    "\n",
    "#test case\n",
    "#fdfe = fdf[0:1000]\n",
    "#for idx in range(0, 1000, 10):\n",
    "\n",
    "for idx in range(0, z_len-10, 10):\n",
    "    if (idx % 1000) == 0:\n",
    "        txt=str(idx)+' of '+str(z_len)\n",
    "        print(txt)\n",
    "    \n",
    "    #get start and end points for this iteration\n",
    "    st=idx\n",
    "    ed=idx+10\n",
    "\n",
    "    this=df.iloc[st:ed]\n",
    "\n",
    "    this_plastic_fraction = np.asarray(df.iloc[st:ed]['plastic_threshold'])   \n",
    "    this_plastic_uncertainty = np.asarray(df.iloc[st:ed]['plastic_unc_median'])   \n",
    "\n",
    "    this_detect_prob, fraction_meeting_threshold = \\\n",
    "        detect_prob_all(this_plastic_uncertainty,this_plastic_fraction,fraction_threshold=0.95)\n",
    "\n",
    "    fdfe = fdfe.append(df.iloc[st], ignore_index = True)\n",
    "        \n",
    "    fdfe.loc[fdfe.index[-1], 'plastic_threshold']= fraction_meeting_threshold\n",
    "    \n",
    "    #section to update field with median values for set\n",
    "    fdfe.loc[fdfe.index[-1], 'plastic_unc_median']= np.median(this_plastic_uncertainty)\n",
    "    fdfe.loc[fdfe.index[-1], 'SIC_median']= np.median(np.asarray(df.iloc[st:ed]['SIC_median']))\n",
    "    fdfe.loc[fdfe.index[-1], 'plastic_avgK_median']= np.median(np.asarray(df.iloc[st:ed]['plastic_avgK_median']))\n",
    "    fdfe.loc[fdfe.index[-1], 'DFS_median']= np.median(np.asarray(df.iloc[st:ed]['DFS_median']))\n",
    "    \n",
    "\n",
    "\n",
    "#change names in original df back\n",
    "df.rename(columns={'plastic_threshold':'plastic_fraction', 'plastic_unc_median':'plastic_uncertainty', 'SIC_median':'SIC', \\\n",
    "                   'plastic_avgK_median':'plastic_avgK', 'DFS_median':'DFS'}, inplace=True)\n",
    "\n",
    "\n",
    "fdfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save file or restart from here:\n",
    "\n",
    "# save dataframe to pickle file\n",
    "df.to_pickle('data/SQOOP_df_Amir_v4.pkl')\n",
    "fdfe.to_pickle('data/SQOOP_fdfe_Amir_v4.pkl') \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
